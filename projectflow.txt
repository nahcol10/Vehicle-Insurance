# Vehicle Insurance Project - Complete Implementation Guide

## Project Setup and Structure
1. **Project Template Generation**
   - Execute `template.py` to create the initial project structure
   - LOGIC: This creates a standard, consistent directory structure for ML projects
   - BEST PRACTICE: Using templates ensures all required components are included and follows software engineering principles

2. **Package Configuration**
   - Configure `setup.py` and `pyproject.toml` for local package imports
   - LOGIC: These files help Python recognize your source code as installable packages
   - BEST PRACTICE: Proper packaging enables modular code development and easier imports
   - Reference detailed instructions in `crashcourse.txt`

3. **Environment Setup**
   ```bash
   conda create -n vehicle python=3.10 -y
   conda activate vehicle
   pip install -r requirements.txt
   pip list  # Verify local packages are installed
   ```
   - LOGIC: Isolated environment prevents dependency conflicts
   - BEST PRACTICE: Always specify Python version explicitly and document all dependencies

## MongoDB Setup and Configuration
4. **MongoDB Atlas Setup**
   - Sign up for MongoDB Atlas and create a new project
   - LOGIC: MongoDB provides a scalable, document-based NoSQL database ideal for storing ML datasets
   - BEST PRACTICE: Cloud-hosted databases eliminate infrastructure management concerns

5. **Database Deployment**
   - Choose M0 free tier service
   - Setup username/password for database access
   - LOGIC: Starting with free tier allows scaling later when needed
   - BEST PRACTICE: Use strong passwords and store them securely

6. **Network Access Configuration**
   - Configure IP whitelist with "0.0.0.0/0" for development
   - LOGIC: This allows connections from any IP address
   - BEST PRACTICE: For production, restrict to specific IPs for security

7. **Connection String Setup**
   - Obtain connection string from MongoDB Atlas
   - LOGIC: Connection string contains all parameters needed to establish a database connection
   - BEST PRACTICE: Never hardcode connection strings in source code; use environment variables

8. **Data Upload to MongoDB**
   - Create notebooks directory and upload dataset
   - Create `mongoDB_demo.ipynb` to push data to MongoDB
   - LOGIC: Storing data in MongoDB provides flexibility for data access patterns
   - BEST PRACTICE: Use batch operations for efficient uploads of large datasets

## Logging, Exception Handling and EDA
9. **Logger Implementation**
   - Create logger module and test in `demo.py`
   - LOGIC: Logging provides visibility into application behavior
   - BEST PRACTICE: Use different log levels (INFO, ERROR, DEBUG) appropriately

10. **Exception Handling**
    - Create custom exception classes and test in `demo.py`
    - LOGIC: Custom exceptions improve error traceability and debugging
    - BEST PRACTICE: Include detailed context in exception messages

11. **Exploratory Data Analysis**
    - Add EDA and Feature Engineering notebooks
    - LOGIC: Understanding data distribution and relationships is crucial for model development
    - BEST PRACTICE: Document all insights and reasoning behind feature engineering decisions

## Data Pipeline Components
12. **Data Ingestion Pipeline**
    - Configure constants in `__init__.py`
    - Implement MongoDB connection in `mongo_db_connections.py`
    - Create data access layer in `proj1_data.py`
    - Define entity classes in `config_entity.py` and `artifact_entity.py`
    - Implement data ingestion component
    - LOGIC: Modular design separates concerns and improves maintainability
    - BEST PRACTICE: Use configuration classes to manage pipeline parameters

13. **MongoDB Connection Environment Setup**
    - For Bash:
      ```bash
      export MONGODB_URL="mongodb+srv://<username>:<password>......"
      echo $MONGODB_URL  # Verify
      ```
    - For PowerShell:
      ```powershell
      $env:MONGODB_URL = "mongodb+srv://<username>:<password>......"
      echo $env:MONGODB_URL  # Verify
      ```
    - For Windows Environment Variables:
      - Add MONGODB_URL to system environment variables
    - LOGIC: Environment variables protect sensitive credentials
    - BEST PRACTICE: Never commit credentials to version control

14. **Data Validation Component**
    - Update `main_utils.py` with validation functions
    - Create schema definition in `schema.yaml`
    - Implement data validation component
    - LOGIC: Validates that incoming data matches expected schema
    - BEST PRACTICE: Fail early if data doesn't match expectations

15. **Data Transformation Component**
    - Add `estimator.py` to entity folder
    - Implement transformation logic
    - LOGIC: Consistent preprocessing ensures model reliability
    - BEST PRACTICE: Create reusable transformation pipelines

16. **Model Trainer Component**
    - Update estimator with model training classes
    - Implement trainer component
    - LOGIC: Encapsulates model training process
    - BEST PRACTICE: Make hyperparameters configurable

## AWS Integration for Model Registry
17. **AWS IAM Setup**
    - Create IAM user with AdministratorAccess
    - Generate and download access keys
    - LOGIC: IAM provides secure access to AWS services
    - BEST PRACTICE: Follow principle of least privilege in production

18. **Environment Variable Configuration**
    - Set AWS credentials as environment variables:
      ```bash
      # For Bash
      export AWS_ACCESS_KEY_ID="AWS_ACCESS_KEY_ID"
      export AWS_SECRET_ACCESS_KEY="AWS_SECRET_ACCESS_KEY"
      
      # For PowerShell
      $env:AWS_ACCESS_KEY_ID="AWS_ACCESS_KEY_ID"
      $env:AWS_SECRET_ACCESS_KEY="AWS_SECRET_ACCESS_KEY"
      ```
    - LOGIC: Environment variables keep credentials out of code
    - BEST PRACTICE: Rotate access keys regularly

19. **AWS S3 Configuration**
    - Update constants with AWS configuration
    - Implement AWS connection in `aws_connection.py`
    - Create S3 bucket for model storage
    - LOGIC: S3 provides durable, scalable storage for models
    - BEST PRACTICE: Use versioning on S3 buckets for model history

20. **S3 Estimator Implementation**
    - Create `s3_estimator.py` for model storage operations
    - LOGIC: Abstracts S3 operations for model persistence
    - BEST PRACTICE: Implement retry logic for network operations

## Model Evaluation and Deployment
21. **Model Evaluation Implementation**
    - Implement model evaluation component
    - LOGIC: Compares new models against production baseline
    - BEST PRACTICE: Use multiple evaluation metrics

22. **Model Pusher Implementation**
    - Implement model pusher to move models to production
    - LOGIC: Automates model deployment process
    - BEST PRACTICE: Include model metadata with each deployed model

23. **Prediction Pipeline**
    - Create prediction pipeline structure
    - Setup Flask application in `app.py`
    - LOGIC: Prediction pipeline handles real-time inference
    - BEST PRACTICE: Ensure error handling for all edge cases

24. **Web Application Setup**
    - Add static and template directories
    - LOGIC: Provides user interface for model interaction
    - BEST PRACTICE: Keep UI and business logic separate

## CI/CD Setup
25. **Docker Configuration**
    - Create Dockerfile and .dockerignore
    - LOGIC: Containerization ensures consistent runtime environment
    - BEST PRACTICE: Use multi-stage builds for smaller images

26. **GitHub Actions Workflow**
    - Create AWS deployment workflow in `.github/workflows/aws.yaml`
    - LOGIC: Automates testing and deployment process
    - BEST PRACTICE: Include linting and tests in CI pipeline

27. **ECR Repository Setup**
    - Create ECR repository for Docker images
    - LOGIC: Provides secure storage for container images
    - BEST PRACTICE: Implement image scanning for security

28. **EC2 Instance Setup**
    - Launch EC2 instance for deployment
    - Configure security group
    - LOGIC: EC2 provides scalable compute for hosting
    - BEST PRACTICE: Use auto-scaling groups for production

29. **Docker Installation on EC2**
    ```bash
    sudo apt-get update -y
    sudo apt-get upgrade
    curl -fsSL https://get.docker.com -o get-docker.sh
    sudo sh get-docker.sh
    sudo usermod -aG docker ubuntu
    newgrp docker
    ```
    - LOGIC: Docker enables consistent deployments
    - BEST PRACTICE: Keep Docker up-to-date for security patches

30. **GitHub Self-hosted Runner Setup**
    - Configure self-hosted runner on EC2
    - LOGIC: Self-hosted runners provide direct deployment access
    - BEST PRACTICE: Use dedicated runners for production deployments

31. **GitHub Secrets Configuration**
    - Add secrets for AWS credentials:
      - AWS_ACCESS_KEY_ID
      - AWS_SECRET_ACCESS_KEY
      - AWS_DEFAULT_REGION
      - ECR_REPO
    - LOGIC: Secrets enable secure CI/CD operations
    - BEST PRACTICE: Rotate secrets regularly

32. **EC2 Port Configuration**
    - Configure inbound rules to allow traffic on port 5080
    - LOGIC: Opens required ports for web application access
    - BEST PRACTICE: Allow only necessary ports

33. **Application Testing**
    - Access application via EC2 public IP on port 5080
    - Test training endpoint at /training route
    - LOGIC: Verifies end-to-end functionality
    - BEST PRACTICE: Implement health check endpoints

## Additional Best Practices
- Implement comprehensive unit and integration tests
- Set up monitoring and alerting for production deployments
- Document API endpoints and expected inputs/outputs
- Create infrastructure as code using Terraform or CloudFormation
- Implement blue-green deployment strategy for zero-downtime updates
- Regularly audit security configurations and dependencies
